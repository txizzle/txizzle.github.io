<!DOCTYPE HTML>
<html>
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9KDC3Z8917"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-9KDC3Z8917');
    </script>

    <!-- Title -->
    <title>Ted Xiao - Google DeepMind</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=1000">

    <!-- Isotope JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.13.2/jquery-ui.min.js"></script>
    <script src="https://unpkg.com/isotope-layout@3/dist/isotope.pkgd.min.js"></script>

    <!-- Custom Style -->
    <link rel="stylesheet" href="style.css">

    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
    <style>
      @import url('https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');
    </style>
  </head>

  <body id="body">

    <div id="main">
      <div id="intro">
        <div id="intro-text">
          <h1>Ted Xiao</h1>
          <p>
            I'm a research scientist at <a href="https://research.google/teams/robotics/">Google DeepMind</a>, where I work on making robots smarter. My research focuses on robot learning, internet-scale foundation models, and reinforcement learning. I am particularly interested in methods that can scale and generalize in the real world. 
            <div id="more-bio" style="display: None">
              <br>
              <p>Ted Xiao is a Senior Research Scientist at <a href="https://research.google/teams/robotics/">Google DeepMind</a> working on robot learning. His research agenda focuses on scaling robot learning in the real world, with a particular focus on approaches that can leverage internet-scale foundation models and methods that improve with more experience. Prior to joining Google DeepMind, Ted founded <a href="https://ml.berkeley.edu/">Machine Learning at Berkeley</a> and worked at Adobe Research. Ted received his B.S. and M.S. in Electrical Engineering and Computer Science from <a href="https://www.berkeley.edu/">UC Berkeley</a>, where he was advised by Professor Claire Tomlin.  </p>
            </div>
            <br>
            <a href="javascript:toggle_bio()">Formal Bio</a>&nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en">G. Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://twitter.com/xiao_ted">Twitter</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://agentic.substack.com/">Blog</a>&nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://www.linkedin.com/in/tedxiao/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
            <!-- <a href="https://github.com/txizzle">GitHub</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
            <br><br>
            xiaoted at gmail dot com
            <br><br>
          </p>
        </div>
        <div id="intro-image">
          <img src="images/profile.jpeg">
        </div>
      </div>

      <div id="filters" class="button-group">
        <!-- <button class="button" data-filter="*">Show All</button> -->
        <button class="button is-checked" data-filter=".highlight">Highlights</button>
        <button class="button" data-filter=".publication">Research</button>
        <button class="button" data-filter=".talk">Talks</button>
        <button class="button" data-filter=".writing">Writing</button>
        <button class="button" data-filter=".misc">Misc</button>
      </div>

      <div class="grid">

        <!-- Highlights -->
        <div class="list-item highlight description" data-category="highlight">
          Some recent highlights:
        </div>

        <!-- Preview Videos -->
        <div class="list-item highlight previews" data-category="highlight">

          <a href="https://robotics-transformer2.github.io/">
            <p>RT-2</p>
            <br>
            <video class="preview1" playsinline="" muted="" autoplay="" loop="">
              <source src="images/video-rt2.mp4" type="video/mp4">
            </video>
          </a>

          <a href="https://robotics-transformer.github.io/">
            <p>RT-1 and SayCan</p>
            <br>
            <video class="preview2" playsinline="" muted="" autoplay="" loop="">
              <source src="images/video-saycan-rt1.mp4" type="video/mp4">
            </video>
          </a>
          <!-- </a> -->

        <a href="https://innermonologue.github.io//">
          <p>Inner Monologue</p>
          <br>
          <video class="preview3" playsinline="" muted="" autoplay="" loop="">
            <source src="images/video-inner-monologue.mp4" type="video/mp4">
          </video>
        </a>
          

        </div>

        <!-- Truncated Set of Highlights (Shown by Default) -->
        <div id="main-highlights">

          <div class="list-item highlight" data-category="highlight">
            <b>Google DeepMind</b> blog post <a href="https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/">"Shaping the Future of Advanced Robotics"</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>TechCrunch</b> article <a href="https://techcrunch.com/2024/01/04/google-outlines-new-methods-for-training-robots-with-video-and-large-language-models/">"Google Outlines New Methods for Training Robots with Video and Large Language Models"</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>New York Times</b> article <a href="https://www.nytimes.com/2023/07/28/technology/google-robots-ai.html">"With the Aid of A.I., Google's Robots Get Smart"</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>Google DeepMind</b> blog post <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">"Scaling Up Learning Across Many Different Robot Types"</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>CNET</b> video <a href="https://www.youtube.com/watch?v=dCPHGwW9SOk">"Googleâ€™s Most Advanced Robot Brain Just Got a Body"</a>
          </div>
          
          <div class="list-item highlight" data-category="highlight">
            <b>CoRL</b> <a href="https://corl2022.org/awards/">Special Innovation Award</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>Google AI</b> blog post <a href="https://blog.research.google/2022/04/efficiently-initializing-reinforcement.html">"Efficiently Initializing Reinforcement Learning With Prior Policies"</a>
          </div>

          <div class="list-item highlight" data-category="highlight">
            <b>60 Minutes</b> video <a href="https://www.cbsnews.com/news/google-artificial-intelligence-future-60-minutes-transcript-2023-04-16/">"Is Artificial Intelligence Advancing Too Quickly?"</a>
          </div>
        </div>

        <!-- Publications -->
        <div class="list-item publication" data-category="publication">
          <a href="https://homangab.github.io/gen2act/" class="thumbnail">
            <img src="images/gen2act_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3>Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation</h3>
            <p>Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, Sean Kirmani<br>
                <i>Preprint</i><br>
                <a href="https://homangab.github.io/gen2act/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2409.16283">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=bZLiYzAKi8U">Video</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://lauramsmith.github.io/steer/" class="thumbnail">
            <img src="images/steer_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3>STEER: Flexible Robotic Manipulation via Dense Language Grounding</h3>
            <p>Laura Smith, Alex Irpan, Montserrat Gonzalez Arenas, Sean Kirmani, Dmitry Kalashnikov, Dhruv Shah, Ted Xiao<br>
                <i>International Conference on Robotics and Automation (ICRA) 2025</i><br>
                <a href="https://lauramsmith.github.io/steer/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2411.03409">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://snasiriany.me/rt-affordance" class="thumbnail">
            <img src="images/rta_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3>RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation</h3>
            <p>Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura Smith, Yuke Zhu, Danny Driess, Dorsa Sadigh, Ted Xiao<br>
                <i>International Conference on Robotics and Automation (ICRA) 2025</i><br>
                <a href="https://snasiriany.me/rt-affordance">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2411.02704">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://generative-value-learning.github.io/" class="thumbnail">
            <img src="images/gvl_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3>Vision Language Models are In-Context Value Learners</h3>
            <p>Yecheng Jason Ma, Joey Hejna, Ayzaan Wahid, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, Jonathan Tompson, Osbert Bastani, Dinesh Jayaraman, Wenhao Yu, Tingnan Zhang, Dorsa Sadigh, Fei Xia<br>
                <i><font color="48bf9">&#9733; Spotlight &#9733;</font> International Conference on Learning Representations (ICLR) 2025</i><br>
                <a href="https://generative-value-learning.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2411.04549">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://generative-value-learning.github.io/#online-demo">Demo</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://openvla.github.io/" class="thumbnail">
            <img src="images/openvla_thumb.jpg" alt="" />
          </a>
          <div class="project-description">
            <h3>OpenVLA: An Open-Source Vision-Language-Action Model</h3>
            <p>Moo Jin Kim*, Karl Pertsch*, Siddharth Karamcheti*, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn<br>
                <i><font color="49bf9">&#9733; Outstanding Paper Award Finalist &#9733;</font> Conference on Robot Learning (CoRL) 2024</i><br>
                <a href="https://openvla.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2406.09246">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/openvla/openvla">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://huggingface.co/openvla">Models</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://rt-sketch.github.io/" class="thumbnail">
            <img src="images/rtsketch_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3>RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches</h3>
            <p>
              Priya Sundaresan, Quan Vuong, Jiayuan Gu, Peng Xu, Ted Xiao, Sean Kirmani, Tianhe Yu, Michael Stark, Ajinkya Jain, Karol Hausman, Dorsa Sadigh*, Jeannette Bohg*, Stefan Schaal*<br>
                <i><font color="48bf9">&#9733; Oral Presentation &#9733;</font> Conference on Robot Learning (CoRL) 2024</i><br>
                <a href="https://rt-sketch.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://rt-sketch.github.io/assets/rt_sketch.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://simpler-env.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-simpler.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3>Evaluating Real-World Robot Manipulation Policies in Simulation</h3>
            <p>Xuanlin Li*, Kyle Hsu*, Jiayuan Gu*, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su*, Quan Vuong*, Ted Xiao*<br>
                <i>Conference on Robot Learning (CoRL) 2024</i><br>
                <a href="https://simpler-env.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2405.05941">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/simpler-env/SimplerEnv">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://colab.research.google.com/github/simpler-env/SimplerEnv/blob/main/example.ipynb">Colab</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://rt-hierarchy.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rth.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3>RT-H: Action Hierarchies Using Language</h3>
            <p>Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa Sadigh<br>
                <i>Robotics: Science and Systems (RSS) 2024</i><br>
                <a href="https://rt-hierarchy.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2403.01823.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://octo-models.github.io/" class="thumbnail">
            <img src="images/octo_thumb.jpg" alt="" />
          </a>
          <div class="project-description">
            <h3>Octo: An Open-Source Generalist Robot Policy</h3>
            <p>
              Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine<br>
                <i>Robotics: Science and Systems (RSS) 2024</i><br>
                <a href="https://octo-models.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2405.12213">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/octo-models/octo">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://huggingface.co/rail-berkeley">Model Weights</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://iliad.stanford.edu/robot-data-comp/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-datacomp.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3>Efficient Data Collection for Robotic Manipulation via Compositional Generalization</h3>
            <p>Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa Sadigh<br>
                <i>Robotics: Science and Systems (RSS) 2024</i><br>
                <a href="https://iliad.stanford.edu/robot-data-comp/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2403.05110.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/1kPQ5Pcn0uQTEO3UeNPwjP9SEueZQ1d03/view">Video</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://arxiv.org/pdf/2403.03950.pdf" class="thumbnail">
            <img src="images/stop_regressing_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3>Stop Regressing: Training Value Functions via Classification for Scalable Deep RL</h3>
            <p>
              Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali TaÃ¯ga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, Rishabh Agarwal<br>
                <i>International Conference on Machine Learning (ICML) 2024</i><br>
                <a href="https://arxiv.org/pdf/2403.03950.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://pivot-prompt.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-pivot.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://pivot-prompt.github.io/">PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs</a></h3>
            <p>
              Soroush Nasiriany*, Fei Xia*, Wenhao Yu*, Ted Xiao*, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei, Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, Brian Ichter*<br>
                <i>International Conference on Machine Learning (ICML) 2024</i><br>
                <a href="https://pivot-prompt.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2402.07872.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://pivot-prompt.github.io/#demo">Demo</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://robot-teaching.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-lmpc.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://robot-teaching.github.io/">Learning to Learn Faster from Human Feedback with Language Model Predictive Control</a></h3>
            <p>
              Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed, Chuyuan Kelly Fu, Nimrod Gileadi, Marissa Giustina, Keerthana Gopalakrishnan, Leonard Hasenclever, Jan Humplik, Jasmine Hsu, Nikhil Joshi, Ben Jyenis, Chase Kew, Sean Kirmani, Tsang-Wei Edward Lee, Kuang-Huei Lee, Assaf Hurwitz Michaely, Joss Moore, Ken Oslund, Dushyant Rao, Allen Ren, Baruch Tabanpour, Quan Vuong, Ayzaan Wahid, Ted Xiao, Ying Xu, Vincent Zhuang, Peng Xu, Erik Frey, Ken Caluwaerts, Tingnan Zhang, Brian Ichter, Jonathan Tompson, Leila Takayama, Vincent Vanhoucke, Izhak Shafran, Maja Mataric, Dorsa Sadigh, Nicolas Heess, Kanishka Rao, Nik Stewart, Jie Tan, Carolina Parada<br>
                <i>Robotics: Science and Systems (RSS) 2024</i><br>
                <a href="https://robot-teaching.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2402.11450.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://colab.research.google.com/drive/1YcRN_kklw3cVVJNvgK_IEV6nDce9EJWK?usp=sharing">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://robot-teaching.github.io/#demo">Demo</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://auto-rt.github.io/" class="thumbnail">
            <img src="images/autort_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://auto-rt.github.io/">AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents</a></h3>
            <p>
              Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu<br>
                <i>Preprint</i><br>
                <a href="https://auto-rt.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2401.12963.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://iliad.stanford.edu/pg-vlm/" class="thumbnail">
            <img src="images/pgv_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://iliad.stanford.edu/pg-vlm/">Physically Grounded Vision-Language Models for Robotic Manipulation</a></h3>
            <p>
              Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh<br>
                <i>IEEE International Conference on Robotics and Automation (ICRA) 2024</i><br>
                <a href="https://iliad.stanford.edu/pg-vlm/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2309.02561.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/17gbzrJSs8YjVafIrX4omR_rx6qLgXjUd/view">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/1ThZ7p_5BnMboK_QE13m1fPKa4WGdRcfC/view">Dataset</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://huggingface.co/bidiptas/PG-InstructBLIP">Model</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://openreview.net/forum?id=1aRNtmy5zX" class="thumbnail">
            <img src="images/promptbook_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://openreview.net/forum?id=1aRNtmy5zX">
              How to Prompt Your Robot: A PromptBook for Manipulation Skills with Code as Policies</a></h3>
            <p>
              Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Z Ren, Quan Vuong, Jake Varley, Alexander Herzog, Isabel Leal, Sean Kirmani, Dorsa Sadigh, Vikas Sindhwani, Kanishka Rao, Jacky Liang, Andy Zeng<br>
                <i>IEEE International Conference on Robotics and Automation (ICRA) 2024,</i><br>
                <i><font color="49bf9">&#9733; Oral Presentation &#9733;</font> Robot Learning Workshop (WRL) at the Conference on Neural Information Processing Systems (NeurIPS) 2023</i><br>
                <a href="https://openreview.net/pdf?id=1aRNtmy5zX">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://robotics-transformer-x.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rtx.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://robotics-transformer-x.github.io/">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a></h3>
            <p>
              Open X-Embodiment Collaboration [>170 Authors]<br>
                <i><font color="49bf9">&#9733; Best Paper Award &#9733;</font>IEEE International Conference on Robotics and Automation (ICRA) 2024</i><br>
                <a href="https://robotics-transformer-x.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2310.08864.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/google-deepmind/open_x_embodiment">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit#gid=0">Dataset</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://sites.google.com/view/generalization-gap/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-gengap.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://sites.google.com/view/generalization-gap/">Decomposing the Generalization Gap in Imitation Learning
              for Visual Robotic Manipulation</a></h3>
            <p>
              Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn<br>
                <i>IEEE International Conference on Robotics and Automation (ICRA) 2024</i><br>
                <i>Robot Learning Workshop (WRL) at the Conference on Neural Information Processing Systems (NeurIPS) 2023</i><br>
                <a href="https://sites.google.com/view/generalization-gap/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2307.03659.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://rt-trajectory.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rttrajectory.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://rt-trajectory.github.io/">RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches</a></h3>
            <p>
              Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao<br>
                <i><font color="49bf9">&#9733; Spotlight &#9733;</font> International Conference on Learning Representations (ICLR) 2024 </i><br>
                <a href="https://rt-trajectory.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2311.01977.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://robot-moo.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-moo.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://robot-moo.github.io/">Open-World Object Manipulation using Pre-Trained Vision-Language Models</a></h3>
            <p>
              Austin Stone*, Ted Xiao*, Yao Lu*, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, Karol Hausman<br>
                <i>Conference on Robot Learning (CoRL) 2023</i><br>
                <a href="https://robot-moo.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2303.00905.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=KyvHTbLRovI&feature=youtu.be">Video</a>
                <br>
            </p>
          </div>
        </div>

        <div class="list-item publication" data-category="publication">
          <a href="https://robotics-transformer2.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rt2.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://robotics-transformer2.github.io/">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></h3>
            <p>
              Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich<br>
                <i>Conference on Robot Learning (CoRL) 2023</i><br>
                <a href="https://robotics-transformer2.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2307.15818.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://robotics-transformer2.github.io/#videos">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://language-to-reward.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-l2r.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://language-to-reward.github.io/">Language to Rewards for Robotic Skill Synthesis</a></h3>
            <p>
              Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien, Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, Fei Xia<br>
              <i><font color="49bf9">&#9733; Oral Presentation &#9733;</font>, Conference on Robot Learning (CoRL) 2023</i><br>
                <a href="https://language-to-reward.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2306.08647.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/google-deepmind/language_to_reward_2023">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://instructionaugmentation.github.io/" class="thumbnail">
            <img src="images/dial_thumb.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://instructionaugmentation.github.io/">Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</a></h3>
            <p>
              Ted Xiao*, Harris Chan*, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, Jonathan Tompson<br>
                <i>Robotics: Science and Systems (RSS) 2023</i><br>
                <a href="https://instructionaugmentation.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2211.11736.pdf">PDF</a>
                <!-- <a href="https://github.com/video-language-planning/vlp_code">Code</a> -->
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://diffusion-rosie.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rosie.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://diffusion-rosie.github.io/">Scaling Robot Learning with Semantically Imagined Experience</a></h3>
            <p>
              Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Dee M, Jodilyn Peralta, Brian Ichter, Karol Hausman, Fei Xia<br>
              <i>Robotics: Science and Systems (RSS) 2023</i><br>
                <a href="https://diffusion-rosie.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2302.11550">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=TRYgNHDS7II&feature=youtu.be">Video</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://rl-at-scale.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-rlatscale.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://rl-at-scale.github.io/">Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators</a></h3>
            <p>
              Alexander Herzog*, Kanishka Rao* Karol Hausman*, Yao Lu*, Paul Wohlhart*, Mengyuan Yan, Jessica Lin, Montserrat Gonzalez Arenas, Ted Xiao, Daniel Kappler, Daniel Ho, Jarek Rettinghouse, Yevgen Chebotar, Kuang-Huei Lee, Keerthana Gopalakrishnan, Ryan Julian, Adrian Li, Chuyuan Kelly Fu, Bob Wei, Sangeetha Ramesh, Khem Holden, Kim Kleiven, David Rendleman, Sean Kirmani, Jeff Bingham, Jon Weisz, Ying Xu, Wenlong Lu, Matthew Bennice, Cody Fong, David Do, Jessica Lam, Yunfei Bai, Benjie Holson, Michael Quinlan, Noah Brown, Mrinal Kalakrishnan, Julian Ibarz, Peter Pastor, Sergey Levine<br>
              <i>Robotics: Science and Systems (RSS) 2023</i><br>
                <a href="https://rl-at-scale.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2305.03270.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=CHS9HSb1uqA">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2023/04/robotic-deep-rl-at-scale-sorting-waste.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://robotics-transformer1.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/rt1_thumb.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://robotics-transformer1.github.io/">RT-1: Robotics Transformer for Real-World Control at Scale</a></h3>
            <p>
              Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich<br>
              <i>Robotics: Science and Systems (RSS) 2023</i><br>
                <a href="https://robotics-transformer1.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2212.06817.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=UuKAp9a6wMs">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/google-research/robotics_transformer">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2022/12/rt-1-robotics-transformer-for-real.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://arxiv.org/abs/2211.09119" class="thumbnail">
            <img src="images/ttm_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://arxiv.org/abs/2211.09119">Token Turing Machines</a></h3>
            <p>
              Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, Anurag Arnab<br>
                <i>Conference on Computer Vision and Pattern Recognition (CVPR) 2023</i><br>
                <a href="https://arxiv.org/abs/2211.09119">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing">Code</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://jumpstart-rl.github.io/" class="thumbnail">
            <img src="images/jsrl_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://jumpstart-rl.github.io/">Jump-Start Reinforcement Learning</a></h3>
            <p>
              Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, JosÃ©phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, Karol Hausman<br>
                <i>International Conference on Machine Learning (ICML) 2023</i><br>
                <a href="https://jumpstart-rl.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2204.02372.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2022/04/efficiently-initializing-reinforcement.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://innermonologue.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-inner-monologue.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://innermonologue.github.io/">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></h3>
            <p>
              Wenlong Huang*, Fei Xia*, Ted Xiao*, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter<br>
                <i>Conference on Robot Learning (CoRL) 2022</i><br>
                <a href="https://innermonologue.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2207.05608.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=0sJjdxn5kcI">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=Ybk8hxKeMYQ">2 Minute Papers</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://say-can.github.io/" class="thumbnail">
            <video playsinline="" muted="" autoplay="" loop="" width="180px">
              <source src="images/video-saycan-rt1.mp4" type="video/mp4">
            </video>
          </a>
          <div class="project-description">
            <h3><a href="https://say-can.github.io/">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></h3>
            <p>
              Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng<br>
                <i><font color="49bf9">&#9733; Oral Presentation, Special Innovation Award &#9733;</font>, Conference on Robot Learning (CoRL) 2022</i><br>
                <a href="https://say-can.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2204.01691.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=ysFav0b472w">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://github.com/google-research/google-research/tree/master/saycan">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <!-- <a href="https://sites.research.google/palm-saycan">Demo</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                <a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html">Google AI Blog</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.wired.com/story/google-robot-learned-to-take-orders-by-scraping-the-web/">Wired</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.washingtonpost.com/video/technology/google-is-training-robots-to-perform-complex-tasks/2022/08/16/3339cdbb-344b-482f-8671-33022725df81_video.html">Washington Post</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=dCPHGwW9SOk">CNET</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://piqtopt.github.io/" class="thumbnail">
            <img src="images/piqtopt_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://piqtopt.github.io/">PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale</a></h3>
            <p>
              Kuang-Huei Lee, Ted Xiao, Adrian Li, Paul Wohlhart, Ian Fischer, Yao Lu<br>
              <i>Conference on Robot Learning (CoRL) 2022</i><br>
                <a href="https://piqtopt.github.io/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2210.08217.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=ioMplI5HlZQ">Video</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://arxiv.org/pdf/2111.03189.pdf" class="thumbnail">
            <img src="images/vfs_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://arxiv.org/pdf/2111.03189.pdf">Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning</a></h3>
            <p>
              Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, Brian Ichter<br>
                <i>International Conference on Learning Representations (ICLR) 2022</i><br>
                <a href="https://arxiv.org/pdf/2111.03189.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2022/04/extracting-skill-centric-state.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://awopt.github.io/" class="thumbnail">
            <img src="images/awopt_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://awopt.github.io/">AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale</a></h3>
            <p>
              Yao Lu, Karol Hausman, Yevgen Chebotar, Mengyuan Yan, Eric Jang, Alexander Herzog, Ted Xiao, Alex Irpan, Mohi Khansari, Dmitry Kalashnikov, Sergey Levine<br>
                <i>Conference on Robot Learning (CoRL) 2021</i><br>
                <a href="https://awopt.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2111.05424.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://actionable-models.github.io" class="thumbnail">
            <img src="images/am_gif.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://actionable-models.github.io">Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills</a></h3>
            <p>
              Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, Sergey Levine<br>
                <i>International Conference on Machine Learning (ICML) 2021</i><br>
                <a href="https://actionable-models.github.io">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2104.07749.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=S3SCR7iYMGA">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://blog.research.google/2021/04/multi-task-robotic-reinforcement.html">Blogpost</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://sites.google.com/view/thinkingwhilemoving" class="thumbnail">
            <img src="images/twm_thumb.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://sites.google.com/view/thinkingwhilemoving">Thinking While Moving: Deep Reinforcement Learning with Concurrent Control</a></h3>
            <p>
              Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman*, Alexander Herzog*<br>
                <i>International Conference on Learning Representations (ICLR) 2020</i><br>
                <a href="https://sites.google.com/view/thinkingwhilemoving">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/2004.06089.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=FPqgYJZgq-g">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://www.youtube.com/watch?v=pZyxlf6l0N8">Yannic Kilcher</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://learning-from-play.github.io/" class="thumbnail">
            <img src="images/lfp_thumb.gif" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://learning-from-play.github.io/">Learning Latent Plans from Play</a></h3>
            <p>
              Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet<br>
                <i><font color="49bf9">&#9733; Oral Presentation&#9733;</font>, Conference on Robot Learning (CoRL) 2019</i><br>
                <a href="https://learning-from-play.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                <a href="https://arxiv.org/pdf/1903.01973.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://www.routledge.com/Artificial-Intelligence-Safety-and-Security/Yampolskiy/p/book/9780815369820" class="thumbnail">
            <img src="images/book_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://www.routledge.com/Artificial-Intelligence-Safety-and-Security/Yampolskiy/p/book/9780815369820">Adversarial Machine Learning</a></h3>
            <p>
              Phillip Kuznetsov, Riley Edmunds, Ted Xiao, Humza Iqbal, Raul Puri, Noah Golmant, Shannon Shih<br>
                <i>Contributed Chapter, Artificial Intelligence Safety and Security (CRC Press)</i><br>
                <a href="https://www.routledge.com/Artificial-Intelligence-Safety-and-Security/Yampolskiy/p/book/9780815369820">Book</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://arxiv.org/pdf/1703.09260.pdf" class="thumbnail">
            <img src="images/adobo_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://arxiv.org/pdf/1703.09260.pdf">Goal-Driven Dynamics Learning via Bayesian Optimization</a></h3>
            <p>
              Somil Bansal, Roberto Calandra, Ted Xiao, Sergey Levine, Claire J. Tomlin<br>
                <i>56th IEEE Conference on Decision and Control (CDC) 2017</i><br>
                <a href="https://arxiv.org/pdf/1703.09260.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://tedxiao.me/pdf/literature_review.pdf" class="thumbnail">
            <img src="images/dynamics_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://tedxiao.me/pdf/literature_review.pdf">A Literature Review of Learning and Optimization Methods Applied to Quadrotor Control</a></h3>
            <p>
              Ted Xiao<br>
                <i>Technical Report</i><br>
                <a href="https://tedxiao.me/pdf/literature_review.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://tedxiao.me/pdf/gans_drl.pdf" class="thumbnail">
            <img src="images/gan_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://tedxiao.me/pdf/gans_drl.pdf">Generative Adversarial Networks for Model-based Reinforcement Learning with Tree Search</a></h3>
            <p>
              Ted Xiao*, Gautham Kesineni*<br>
                <i>Technical Report</i><br>
                <a href="https://tedxiao.me/pdf/gans_drl.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>
        <div class="list-item publication" data-category="publication">
          <a href="https://tedxiao.me/pdf/CS294_Report.pdf" class="thumbnail">
            <img src="images/upscaling_thumb.png" alt="" />
          </a>
          <div class="project-description">
            <h3><a href="https://tedxiao.me/pdf/CS294_Report.pdf">Frame Rate Upscaling with Deep Neural Networks</a></h3>
            <p>
              Ted Xiao, Raul Puri, Gautham Kesineni<br>
                <i>Technical Report</i><br>
                <a href="https://tedxiao.me/pdf/CS294_Report.pdf">PDF</a>
                <br>
            </p>
          </div>
        </div>



        <!-- Talks -->
        <div class="list-item talk" data-category="talk">
          <p class="date">2025</p>MIT, <a href="https://futuretech.mit.edu/">Invited Talk</a>
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date">2024</p>NeurIPS, <a href="https://sites.google.com/view/open-world-agents/home">Workshop on Open-World Agents</a> (<a href="https://agentic.substack.com/p/whats-missing-for-robot-foundation">annotated slides</a>)
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date"></p>CoRL, <a href="https://llhomerobots.github.io/">Lifelong Learning for Home Robots Workshop</a>
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date"></p>CoRL, <a href="https://www.dynsyslab.org/mastering-robot-manipulation-in-a-world-of-abundant-data/">Workshop on Mastering Robot Manipulation in a World of Abundant Data</a>
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date"></p>RSS, <a href="https://sites.google.com/view/gai-hri/home">GenAI-HRI Workshop</a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>University of Washington, Invited Talk
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>CVPR, <a href="https://opendrivelab.com/cvpr2024/workshop/">Foundation Models for Autonomous Systems Workshop
          </a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>ICRA, <a href="https://vlmnm-workshop.github.io/">Vision-Langauge Models for Navigation and Manipulation Workshop Debate</a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>HKU, <a href="https://taoyds.github.io/courses/comp3361">COMP 3361 Guest Lecture</a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>Cognitive Revolution, <a href="https://www.youtube.com/watch?v=ZFi0eOYzHiI">Podcast Guest</a>
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date"></p>Yale, <a href="https://www.vandijklab.org/teaching">CPSC 482/582 Guest Lecture</a>
        </div>
        <div class="list-item talk" data-category="talk">
          <p class="date">2023</p>USC, <a href="https://sites.google.com/usc.edu/cs699-fa23-manipulation/home">CS699 Guest Lecture</a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>UT Austin, <a href="https://www.cs.utexas.edu/~yukez/cs391r_fall2023/">CS391R Guest Lecture</a> (<a href="https://docs.google.com/presentation/d/1Kq3zLKI19u4NC7t21bDt7_nyc-OkPHevrv3EwWTVzCA/edit?usp=sharing&resourcekey=0-Xdy8GqL2PIhlWstJcTcEqg">slides</a>)
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>NVIDIA, Invited Talk
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>Stanford, <a href="https://web.stanford.edu/class/cs25/prev_years/2023_winter/index.html">CS25 Guest Lecture</a> (<a href="https://www.youtube.com/watch?v=ct4tdyyNDY4">recording</a>)
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date">2022</p>UPenn, <a href="https://www.grasp.upenn.edu/events/month/">GRASP SFI Seminar</a> (<a href="https://www.youtube.com/watch?v=t-Gl-a2GAKk">recording</a>)
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date">2021</p>UC Berkeley, <a href="https://ml.berkeley.edu/">Machine Learning at Berkeley</a> Tutorial
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date">2018</p>Xoogler, Machine Learning Keynote
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date">2017</p>RobotX, <a href="https://www.eventbrite.com/e/robotx-workshop-deep-learning-security-nlp-with-uc-berkeley-ai-lab-tickets-33294488712">NLP Tutorial</a>
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>UC Davis, <a href="https://www.facebook.com/iidatascience/">iidata Conference</a> Keynote
        </div>

        <div class="list-item talk" data-category="talk">
          <p class="date"></p>DiversaTech, Technical Interview Workshop
        </div>

        <!-- Writing -->
        <div class="list-item writing" data-category="writing">
          <p class="date">2024</p><a href="https://agentic.substack.com/p/whats-missing-for-robot-foundation">What's Missing for Robot Foundation Models?</a>
        </div>
        <div class="list-item writing" data-category="writing">
          <p class="date">2023</p><a href="https://agentic.substack.com/p/robotics-in-the-era-of-foundation">Robotics in the Era of Foundation Models</a>
        </div>

        <div class="list-item writing" data-category="writing">
          <p class="date"></p><a href="https://agentic.substack.com/p/ai-is-domestication">AI is Domestication</a>
        </div>



        <!-- Services -->
        <div class="list-item misc" data-category="misc">
          <p class="date">2023</p>Co-Organizer, CoRL Workshop on <a href="https://sites.google.com/view/langrob-corl23/home">Language and Robot Learning</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date"></p>Demo, Robotics: Science and Systems (RSS) - <a href="https://roboticsconference.org/program/papers/024/">Large Language Models on Robots</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date"></p>Lead Organizer, ICRA Workshop on <a href="https://sites.google.com/view/ldod2023/home">Learning from Diverse, Offliine Data</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date"></p>Co-Organizer, ICLR Workshop on <a href="https://reincarnating-rl.github.io/">Reincarnating Reinforcement Learning</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date"></p>Co-Organizer, CoRL Workshop on <a href="https://sites.google.com/view/langrob-corl22/">Language and Robot Learning</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date">2022</p>Co-Organizer, NeurIPS <a href="https://sites.google.com/view/deep-rl-workshop-neurips-2022">Deep Reinforcement Learning Workshop</a>
        </div>

        <div class="list-item misc" data-category="misc">
          <p class="date">2024+</p>Area Chair, CoRL
        </div>
        <div class="list-item misc" data-category="misc">
          <p class="date">2018+</p>Reviewer, CoRL, RSS, RA-L, ICRA, NeurIPS, ICML, ICLR
        </div>

      </div>

      <div id="footer">Website template from <a href="https://andyzeng.github.io/">Andy Zeng</a>.</div>

    </div>

    <script>

      // Isotope grid.
      var $grid = $('.grid').isotope({
        itemSelector: '.list-item',
        layoutMode: 'fitRows',
        transitionDuration: 0,
        stagger: 10,
        initLayout: false,
        getSortData: {
          name: '.name',
          symbol: '.symbol',
          number: '.number parseInt',
          category: '[data-category]',
          weight: function( itemElem ) {
            var weight = $( itemElem ).find('.weight').text();
            return parseFloat( weight.replace( /[\(\)]/g, '') );
          }
        }
      });

      // Bind filter button click.
      $('#filters').on( 'click', 'button', function() {
        var filterValue = $( this ).attr('data-filter');
        localStorage.setItem('filterValue', filterValue);
        $grid.isotope({ filter: filterValue });
      });

      // Change is-checked class on buttons.
      $('.button-group').each( function( i, buttonGroup ) {
        var $buttonGroup = $( buttonGroup );
        $buttonGroup.on( 'click', 'button', function() {
          $buttonGroup.find('.is-checked').removeClass('is-checked');
          $( this ).addClass('is-checked');
        });
      });

      function update_isotope() {
        // Retrieve cached button click.
        var defaultFilterValue = localStorage.getItem('filterValue');
        if (defaultFilterValue == null) {
          defaultFilterValue = ".highlight"
        }
        $grid.isotope({ filter: defaultFilterValue });
        var buttons = document.getElementsByClassName("button");
        for (var currButton of buttons) {
          if (currButton.getAttribute('data-filter') == defaultFilterValue) {
            currButton.classList.add('is-checked');
          } else {
            currButton.classList.remove('is-checked');
          }
        }
      }

      function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }

      function toggle_highlights() {
        var x = document.getElementById("main-highlights");
        var y = document.getElementById("more-highlights");
        var b = document.getElementById("toggle_highlights_button")
        if (y.style.display === "none") {
          x.style.display = "none";
          y.style.display = "block";
          b.innerHTML = "Show less"
          update_isotope();
        } else {
          x.style.display = "block";
          y.style.display = "none";
          b.innerHTML = "Show more"
          update_isotope();
        }
      }

      update_isotope();

    </script>
  </body>
</html>
